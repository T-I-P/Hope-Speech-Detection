{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSD_Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMWFmURgWWFu",
        "outputId": "802ecda5-8dbb-4060-df61-38af0f979abe"
      },
      "source": [
        "# Downloading packages\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcZ6yhCFYbon"
      },
      "source": [
        "# Importing necessary libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk import pos_tag\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from collections import defaultdict\r\n",
        "from nltk.corpus import wordnet as wn\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn import model_selection, naive_bayes, svm\r\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jxUSr0gYq12"
      },
      "source": [
        "# Read trainig and validation dataset\r\n",
        "header_list = [\"text\",\"intent\",\"buffer\"]\r\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/T-I-P/Hope-Speech-Detection/master/English/english_hope_train.csv\", '\\t',header=None,names=header_list)\r\n",
        "valid = pd.read_csv(\"https://raw.githubusercontent.com/T-I-P/Hope-Speech-Detection/master/English/english_hope_dev.csv\", '\\t',header=None,names=header_list)\r\n",
        "train = train.drop(labels='buffer',axis=1)\r\n",
        "valid = valid.drop(labels='buffer',axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "uHOS7gyMZB0K",
        "outputId": "dd7280d2-6082-4a48-fa3d-e4f7d75c1701"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>these tiktoks radiate gay chaotic energy and i...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Champions Again He got killed for using false...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's not that all lives don't matter</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is it really that difficult to understand? Bla...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Whenever we say black isn't that racists?  Why...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text           intent\n",
              "0  these tiktoks radiate gay chaotic energy and i...  Non_hope_speech\n",
              "1  @Champions Again He got killed for using false...  Non_hope_speech\n",
              "2               It's not that all lives don't matter  Non_hope_speech\n",
              "3  Is it really that difficult to understand? Bla...  Non_hope_speech\n",
              "4  Whenever we say black isn't that racists?  Why...  Non_hope_speech"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozcGMTzZZfNU"
      },
      "source": [
        "train = train.loc[train['intent']!='not-English']\r\n",
        "valid = valid.loc[valid['intent']!='not-English']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPKx6KPdZjDG",
        "outputId": "2ced6b46-d953-4a5e-898b-ae13c1cb0bb6"
      },
      "source": [
        "print(train.shape, valid.shape)\r\n",
        "print(np.unique(train['intent']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22740, 2) (2841, 2)\n",
            "['Hope_speech' 'Non_hope_speech']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFV-5TIDcjRV"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akovtqQSZk3u"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \r\n",
        "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \r\n",
        "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \r\n",
        "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \r\n",
        "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \r\n",
        "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \r\n",
        "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \r\n",
        "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \r\n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \r\n",
        "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \r\n",
        "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \r\n",
        "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \r\n",
        "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \r\n",
        "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \r\n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \r\n",
        "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \r\n",
        "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \r\n",
        "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \r\n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\r\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \r\n",
        "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \r\n",
        "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \r\n",
        "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \r\n",
        "                   \"this's\": \"this is\",\r\n",
        "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \r\n",
        "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \r\n",
        "                       \"here's\": \"here is\",\r\n",
        "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \r\n",
        "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \r\n",
        "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \r\n",
        "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \r\n",
        "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \r\n",
        "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \r\n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \r\n",
        "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \r\n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \r\n",
        "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \r\n",
        "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \r\n",
        "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \r\n",
        "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\r\n",
        "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\r\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \r\n",
        "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"let 's\": \"let us\", \"'em\": \"them\"}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBKbFVccaeMP",
        "outputId": "6e24e953-142b-4a4d-d399-549c03968e16"
      },
      "source": [
        "pip install Unidecode"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 29.5MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 18.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 17.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjBKqMHNaJ1M"
      },
      "source": [
        "# Code block for data cleaning\r\n",
        "import codecs\r\n",
        "import unidecode\r\n",
        "import re\r\n",
        "import spacy\r\n",
        "nlp = spacy.load('en')\r\n",
        "\r\n",
        "def spacy_cleaner(text):\r\n",
        "    try:\r\n",
        "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\r\n",
        "    except:\r\n",
        "        decoded = unidecode.unidecode(text)\r\n",
        "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\r\n",
        "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\r\n",
        "    parsed = nlp(expanded)\r\n",
        "    final_tokens = []\r\n",
        "    for t in parsed:\r\n",
        "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            if t.lemma_ == '-PRON-':\r\n",
        "                final_tokens.append(str(t))\r\n",
        "            else:\r\n",
        "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\r\n",
        "                if len(sc_removed) > 1:\r\n",
        "                    final_tokens.append(sc_removed)\r\n",
        "    joined = ' '.join(final_tokens)\r\n",
        "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\r\n",
        "    return spell_corrected"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5x9De54acoh"
      },
      "source": [
        "train['text'] = [spacy_cleaner(t) for t in train.text]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Ar6anaa4uw"
      },
      "source": [
        "valid['text'] = [spacy_cleaner(t) for t in valid.text]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-f-reU_c9zM"
      },
      "source": [
        "# Change all the text to lower case. This is required as python interprets 'python', 'Python\" and 'PYTHON' differently\r\n",
        "train['text'] = [entry.lower() for entry in train['text']]\r\n",
        "valid['text'] = [entry.lower() for entry in valid['text']]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ZuysDAj1c0E0",
        "outputId": "03ab9589-949e-4de8-e8fb-3e26ab626ac5"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>these tiktok radiate gay chaotic energy and lo...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>again he get kill for use false money</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it be not that all life do not matter</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>be it really that difficult to understand blac...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>whenever we say black be not that racist why d...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text           intent\n",
              "0  these tiktok radiate gay chaotic energy and lo...  Non_hope_speech\n",
              "1              again he get kill for use false money  Non_hope_speech\n",
              "2              it be not that all life do not matter  Non_hope_speech\n",
              "3  be it really that difficult to understand blac...  Non_hope_speech\n",
              "4  whenever we say black be not that racist why d...  Non_hope_speech"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B7CN0UZdSnd",
        "outputId": "073241a2-7741-4e62-84fd-7cf6562bb23a"
      },
      "source": [
        "valid.isnull().sum()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text      0\n",
              "intent    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJTUostldtvI"
      },
      "source": [
        "train.to_csv('train_cleaned.csv')\r\n",
        "valid.to_csv('valid_cleaned.csv')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoGO9w4NfV-5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpz4y1HcfViO"
      },
      "source": [
        "## Tokenization / Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rP0JVYCerOq"
      },
      "source": [
        "# Tokenization : In this each entry in the corpus will be broken into set of words\r\n",
        "train['text']= [word_tokenize(entry) for entry in train['text']]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeIbcum-gV_J"
      },
      "source": [
        "# Tokenization : In this each entry in the corpus will be broken into set of words\r\n",
        "valid['text']= [word_tokenize(entry) for entry in valid['text']]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bAYCoQRfCDL"
      },
      "source": [
        "# Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting on training dataset\r\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\r\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\r\n",
        "tag_map['J'] = wn.ADJ\r\n",
        "tag_map['V'] = wn.VERB\r\n",
        "tag_map['R'] = wn.ADV\r\n",
        "for index,entry in enumerate(train['text']):\r\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\r\n",
        "    Final_words = []\r\n",
        "    # Initializing WordNetLemmatizer()\r\n",
        "    word_Lemmatized = WordNetLemmatizer()\r\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\r\n",
        "    for word, tag in pos_tag(entry):\r\n",
        "        # Below condition is to check for Stop words and consider only alphabets\r\n",
        "        if word not in stopwords.words('english') and word.isalpha():\r\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\r\n",
        "            Final_words.append(word_Final)\r\n",
        "    # The final processed set of words for each iteration will be stored in 'text_final'\r\n",
        "    train.loc[index,'text_final'] = str(Final_words)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKDA-jDqf6jf"
      },
      "source": [
        "# Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting on validation dataset\r\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\r\n",
        "tag_map['J'] = wn.ADJ\r\n",
        "tag_map['V'] = wn.VERB\r\n",
        "tag_map['R'] = wn.ADV\r\n",
        "for index,entry in enumerate(valid['text']):\r\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\r\n",
        "    Final_words = []\r\n",
        "    # Initializing WordNetLemmatizer()\r\n",
        "    word_Lemmatized = WordNetLemmatizer()\r\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\r\n",
        "    for word, tag in pos_tag(entry):\r\n",
        "        # Below condition is to check for Stop words and consider only alphabets\r\n",
        "        if word not in stopwords.words('english') and word.isalpha():\r\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\r\n",
        "            Final_words.append(word_Final)\r\n",
        "    # The final processed set of words for each iteration will be stored in 'text_final'\r\n",
        "    valid.loc[index,'text_final'] = str(Final_words)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRWkTZ8Yghw8",
        "outputId": "474bac50-3159-4c2c-875c-6450b71fbcf4"
      },
      "source": [
        "valid.isnull().sum()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text          2\n",
              "intent        2\n",
              "text_final    2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ9g_xpXhBc4"
      },
      "source": [
        "train.dropna(subset=['intent'], inplace=True)\r\n",
        "train.dropna(subset=['text_final'], inplace=True)\r\n",
        "valid.dropna(subset=['intent'], inplace=True)\r\n",
        "valid.dropna(subset=['text_final'], inplace=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiMvUJhfiWkG",
        "outputId": "24de71d1-2400-4e48-a9d0-3f9c75a3c77d"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text          0\n",
              "intent        0\n",
              "text_final    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "HTXGXB9niYuR",
        "outputId": "5d9ec7a2-a8de-4b9f-9c52-d80c4fdfc469"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "      <th>text_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[these, tiktok, radiate, gay, chaotic, energy,...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>['tiktok', 'radiate', 'gay', 'chaotic', 'energ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[again, he, get, kill, for, use, false, money]</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>['get', 'kill', 'use', 'false', 'money']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[it, be, not, that, all, life, do, not, matter]</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>['life', 'matter']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[be, it, really, that, difficult, to, understa...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>['really', 'difficult', 'understand', 'black',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[whenever, we, say, black, be, not, that, raci...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "      <td>['whenever', 'say', 'black', 'racist', 'say', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         text_final\n",
              "0  [these, tiktok, radiate, gay, chaotic, energy,...  ...  ['tiktok', 'radiate', 'gay', 'chaotic', 'energ...\n",
              "1     [again, he, get, kill, for, use, false, money]  ...           ['get', 'kill', 'use', 'false', 'money']\n",
              "2    [it, be, not, that, all, life, do, not, matter]  ...                                 ['life', 'matter']\n",
              "3  [be, it, really, that, difficult, to, understa...  ...  ['really', 'difficult', 'understand', 'black',...\n",
              "4  [whenever, we, say, black, be, not, that, raci...  ...  ['whenever', 'say', 'black', 'racist', 'say', ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcPvio2mipdU"
      },
      "source": [
        "train.to_csv('train_tokenized.csv')\r\n",
        "valid.to_csv('valid_tokenized.csv')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyEO-I2liyY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}